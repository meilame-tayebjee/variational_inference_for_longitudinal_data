{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  4 20:22:30 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A2000 12GB          Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 40%   63C    P8             13W /   70W |    2118MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    312721      C   ...lame.tayebjee/miniconda3/bin/python        368MiB |\n",
      "|    0   N/A  N/A   2230819      G   /usr/libexec/Xorg                              94MiB |\n",
      "|    0   N/A  N/A   2230845      G   /usr/bin/gnome-shell                           34MiB |\n",
      "|    0   N/A  N/A   2244078      C   ...lame.tayebjee/miniconda3/bin/python       1608MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#%cd variational_inference_for_longitudinal_data/\n",
    "sys.path.append('diffusion/stable_diffusion/')\n",
    "sys.path.append('diffusion/stable_diffusion/model/')\n",
    "sys.path.append('lib/src/')\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from diffusion.stable_diffusion.latent_diffusion import MyLatentDiffusion, LitLDM\n",
    "from diffusion.stable_diffusion.model.unet import UNetModel\n",
    "from diffusion.stable_diffusion.sampler.ddim import DDIMSampler\n",
    "\n",
    "from lib.src.pythae.models import VAE\n",
    "from lib.src.pythae.models import LLDM_IAF, LVAE_IAF_Config\n",
    "from lib.src.pythae.trainers import BaseTrainerConfig, BaseTrainer\n",
    "from lib.scripts.utils import Encoder_Chairs,Decoder_Chairs\n",
    "from lib.scripts.utils import My_MaskedDataset, make_batched_masks\n",
    "\n",
    "\n",
    "def load_config_unet(config):\n",
    "    return UNetModel(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        channels=config['channels'],\n",
    "        n_res_blocks=config['n_res_blocks'],\n",
    "        attention_levels=config['attention_levels'],\n",
    "        channel_multipliers=config['channel_multipliers'],\n",
    "        n_heads=config['n_heads'],\n",
    "    )\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 8, 64, 64, 3])\n",
      "torch.Size([8000, 8, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA = 'lib/my_data/sprites/Sprites_train.pt'\n",
    "\n",
    "\n",
    "train_data = torch.load(os.path.join(PATH_DATA))[:-1000]\n",
    "eval_data = torch.load(os.path.join(PATH_DATA), map_location=\"cpu\")[-1000:]\n",
    "#test_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_test.pt'), map_location=\"cpu\")\n",
    "\n",
    "print(train_data.shape)\n",
    "train_data = train_data.permute(0, 1, 4, 2, 3)\n",
    "eval_data = eval_data.permute(0, 1, 4, 2, 3)\n",
    "#test_data = test_data.permute(0, 1, 4, 2, 3)\n",
    "print(train_data.shape)\n",
    "\n",
    "train_seq_mask = torch.ones(train_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "eval_seq_mask = torch.ones(eval_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "#test_seq_mask = torch.ones(test_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "train_pix_mask = torch.ones_like(train_data, requires_grad=False).type(torch.bool)\n",
    "eval_pix_mask = torch.ones_like(eval_data, requires_grad=False).type(torch.bool)\n",
    "#test_pix_mask = torch.ones_like(test_data, requires_grad=False).type(torch.bool)\n",
    "\n",
    "train_dataset = My_MaskedDataset(train_data, train_seq_mask, train_pix_mask)\n",
    "eval_dataset = My_MaskedDataset(eval_data, eval_seq_mask, eval_pix_mask)\n",
    "#test_dataset = My_MaskedDataset(test_data, test_seq_mask, test_pix_mask)\n",
    "\n",
    "\n",
    "NUM_WORKERS = 12\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=200, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(eval_data, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Kmedoids\n",
      "Finding temperature\n",
      "Best temperature found:  1.8156673908233643\n",
      "Building metric\n",
      "Latent dim: 12\n",
      "Number of parameters in the diffusion model:  2223043\n"
     ]
    }
   ],
   "source": [
    "# encoder = Encoder_Chairs(config)\n",
    "# decoder = Decoder_Chairs(config)\n",
    "# vae = LVAE_IAF(config, encoder, decoder)\n",
    "PATH_VAE_FOLDER = 'pre-trained_vae/VAE_training_2024-05-30_17-33-43-latdim12/final_model'\n",
    "PATH_DIFFUSION_CKPT = 'ldm/lightning_logs/version_30/checkpoints/epoch=44-step=900.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "vae = VAE.load_from_folder(PATH_VAE_FOLDER).to(device)\n",
    "vae.eval()\n",
    "_, _ = vae.retrieveG(train_data, verbose = True)\n",
    "\n",
    "\n",
    "# in_channels = 3\n",
    "# out_channels = 3\n",
    "# channels = 32\n",
    "# n_res_blocks = 2\n",
    "# attention_levels = [2]\n",
    "# channel_multipliers = (1, 2, 4)\n",
    "# n_heads = 16\n",
    "\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "channels = 64\n",
    "n_res_blocks = 4\n",
    "attention_levels = [0]\n",
    "channel_multipliers = [1]\n",
    "n_heads = 4\n",
    "\n",
    "unet_config = {\n",
    "    'in_channels': in_channels,\n",
    "    'out_channels': out_channels,\n",
    "    'channels': channels,\n",
    "    'n_res_blocks': n_res_blocks,\n",
    "    'attention_levels': attention_levels,\n",
    "    'channel_multipliers': channel_multipliers,\n",
    "    'n_heads': n_heads,\n",
    "}\n",
    "\n",
    "unet = load_config_unet(unet_config)\n",
    "\n",
    "latent_scaling_factor = 1\n",
    "n_steps = 1000\n",
    "linear_start =  0.00085\n",
    "linear_end = 0.012\n",
    "\n",
    "input_dim = (3, 64, 64)\n",
    "f = 32 #subsampling factor\n",
    "latent_dim = 3* (64 // f) * (64 // f)\n",
    "print('Latent dim:', latent_dim)\n",
    "\n",
    "\n",
    "latent_diffusion = MyLatentDiffusion(unet, latent_scaling_factor, latent_dim, n_steps, linear_start, linear_end)\n",
    "print('Number of parameters in the diffusion model: ', sum(p.numel() for p in latent_diffusion.parameters() if p.requires_grad))\n",
    "\n",
    "model = LitLDM.load_from_checkpoint(PATH_DIFFUSION_CKPT, ldm = latent_diffusion, vae = vae, latent_dim = latent_dim, lr = 6e-4).to(device)\n",
    "diffusion = model.ldm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on  cuda:0\n",
      "Freezing pre-trained VAE and pre-trained LDM...\n",
      "Freezing done.\n",
      "Number of trainable parameters: 3.6e+06\n",
      "Number of total parameters: 6.9e+06\n"
     ]
    }
   ],
   "source": [
    "config = LVAE_IAF_Config(\n",
    "    input_dim=(3, 64, 64),\n",
    "    n_obs_per_ind=train_data.shape[1], #8 for Sprites\n",
    "    latent_dim=latent_dim,\n",
    "    beta=1,\n",
    "    n_hidden_in_made=6,\n",
    "    n_made_blocks=4,\n",
    "    warmup=2,\n",
    "    context_dim=None,\n",
    "    prior='standard',\n",
    "    posterior='iaf',\n",
    "    linear_scheduling_steps=10\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "encoder = Encoder_Chairs(config).to(device)\n",
    "decoder = Decoder_Chairs(config).to(device)\n",
    "ddim_sampler = DDIMSampler(diffusion, n_steps = train_data.shape[1]-1, ddim_eta = 1)\n",
    "zT_samples = torch.load('zT_samples.pt') #shape (1000, 12)\n",
    "\n",
    "\n",
    "lldm = LLDM_IAF(model_config=config, encoder=encoder, decoder=decoder, \n",
    "                pretrained_vae=vae, pretrained_ldm=diffusion, ddim_sampler=ddim_sampler,\n",
    "                precomputed_zT_samples=zT_samples, verbose = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = BaseTrainerConfig(\n",
    "        num_epochs=100,\n",
    "        learning_rate=1e-6,\n",
    "        batch_size=512,\n",
    "        steps_saving=None,\n",
    "        steps_predict=100,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.Adam(lldm.parameters(), lr=training_config.learning_rate, eps=1e-5)\n",
    "\n",
    "### Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[50, 100, 125, 150],\n",
    "    gamma=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = BaseTrainer(\n",
    "            model=lldm,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            training_config=training_config,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model passed sanity check !\n",
      "\n",
      "Created dummy_output_dir/LLDM_IAF_training_2024-06-04_20-28-12. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n",
      "Successfully launched training !\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f10c2a26087480db4b08638697b8a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 1/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f6c3fcb75f4e7f8adda0bf99c83f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 1/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 385.7482\n",
      "Eval loss: 386.5037\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa5b4debd5d4b9d9eb79bd1e39893ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 2/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7fb7e351b9436e9cccf8da5c25a030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 2/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 680.4539\n",
      "Eval loss: 1865.1428\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0a88dd99304cfebcf74ab0fbd5eddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 3/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab81f0f52a94674be04e9cb05d952dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 3/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 776.7308\n",
      "Eval loss: 958.6192\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dbfb0d3bd34b2894fd44e0c88003f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 4/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb37d4e92ef47f88dace5f4380c261c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 4/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 1053.5114\n",
      "Eval loss: 925.493\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a03b03009a34cf186431ed493c764b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 5/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90db34034a2047ea96ae225d13a33372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 5/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 655.3809\n",
      "Eval loss: 405.1302\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2c34b1aae4450193e9d50b0cf3b24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 6/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e39ff27d2e41c0a06c95645604e041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 6/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 1002.5866\n",
      "Eval loss: 1554.6194\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d57dbe68fae46889b541b73b8cb4978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 7/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb0086bb5b449779f07d18276600aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 7/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 1651.2015\n",
      "Eval loss: 382.2829\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985a4c9134a34f9a8d4d516a892ca381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 8/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3938aacf64488d909c950b9968e7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 8/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 1228.4372\n",
      "Eval loss: 1534.7603\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0ada930dae4f0398c6cfc44d02e0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 9/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0607847bccc4d13908953b7e6e1e6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval of epoch 9/100:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Train loss: 1132.664\n",
      "Eval loss: 659.9601\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff2978e8c9a4565ad0fc834adcf9ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training of epoch 10/100:   0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/variational_inference_for_longitudinal_data/lib/src/pythae/trainers/base_trainer/base_trainer.py:345\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, log_output_dir)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_begin(\n\u001b[1;32m    337\u001b[0m     training_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_config,\n\u001b[1;32m    338\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    339\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader,\n\u001b[1;32m    340\u001b[0m     eval_loader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_loader,\n\u001b[1;32m    341\u001b[0m )\n\u001b[1;32m    343\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 345\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_epoch_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m epoch_train_loss\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/variational_inference_for_longitudinal_data/lib/src/pythae/trainers/base_trainer/base_trainer.py:501\u001b[0m, in \u001b[0;36mBaseTrainer.train_step\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    495\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_inputs_to_device(inputs)\n\u001b[1;32m    497\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    498\u001b[0m     inputs, epoch\u001b[38;5;241m=\u001b[39mepoch, dataset_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    499\u001b[0m )\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizers_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m loss \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    505\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/variational_inference_for_longitudinal_data/lib/src/pythae/trainers/base_trainer/base_trainer.py:236\u001b[0m, in \u001b[0;36mBaseTrainer._optimizers_step\u001b[0;34m(self, model_output)\u001b[0m\n\u001b[1;32m    233\u001b[0m loss \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 236\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m#if self.training_config.gradient_clip is not None:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m#    torch.nn.utils.clip_grad_norm(self.model.parameters(), self.training_config.gradient_clip)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "#ergonomiser le Riemaniann sampling\n",
    "- inspecter LVIAF\n",
    "- train !\n",
    "\n",
    "Possible pain points : le diffusion backward très bof (at least dans les intemédiaires). Après c'est du sampling... wait and see\n",
    "\n",
    "30/05\n",
    "Le problème du vanishing G a été résolu en passant à une dim latente de 12.\n",
    "Il faut améliorer le diffusion model !\n",
    "Eventuellemnt passer à du 16, puis 1, 4, 4 pour avoir des étages ce qui  n'est pas le cas now. Checker si on a pas de vansihing G.\n",
    "\n",
    "\n",
    "04/06 Prêt à train (je crois ?) Il faudrait probablement tester les priors (à voir cmt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'variational_inference_for_longitudinal_data/'\n",
      "/home/mtayebje/variational_inference_for_longitudinal_data\n",
      "Thu May 30 14:51:56 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.02              Driver Version: 545.29.02    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Quadro RTX 8000                On  | 00000000:25:00.0 Off |                    0 |\n",
      "| N/A   23C    P8              24W / 250W |      0MiB / 46080MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "%cd variational_inference_for_longitudinal_data/\n",
    "sys.path.append('diffusion/stable_diffusion/')\n",
    "sys.path.append('diffusion/stable_diffusion/model/')\n",
    "sys.path.append('lib/src/')\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from diffusion.stable_diffusion.latent_diffusion import LatentDiffusion, DiffusionWrapper\n",
    "from diffusion.stable_diffusion.model.autoencoder import Autoencoder\n",
    "from diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder\n",
    "from diffusion.stable_diffusion.model.unet import UNetModel, _test_time_embeddings\n",
    "\n",
    "from lib.src.pythae.models import VAE\n",
    "from lib.src.pythae.models import LVAE_IAF, LVAE_IAF_Config, AutoModel\n",
    "from lib.src.pythae.models.nn.base_architectures import BaseDecoder\n",
    "from lib.scripts.utils import Encoder_Chairs,Decoder_Chairs, My_Dataset, My_MaskedDataset, make_batched_masks\n",
    "\n",
    "def load_config_unet(config):\n",
    "    return UNetModel(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        channels=config['channels'],\n",
    "        n_res_blocks=config['n_res_blocks'],\n",
    "        attention_levels=config['attention_levels'],\n",
    "        channel_multipliers=config['channel_multipliers'],\n",
    "        n_heads=config['n_heads'],\n",
    "    )\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lib/my_data/sprites/Sprites_train.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlib/my_data/sprites/Sprites_train.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m]\n\u001b[1;32m      2\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlib/my_data/sprites/Sprites_train.pt\u001b[39m\u001b[38;5;124m'\u001b[39m), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m:]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#test_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_test.pt'), map_location=\"cpu\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lib/my_data/sprites/Sprites_train.pt'"
     ]
    }
   ],
   "source": [
    "train_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_train.pt'))[:-1000]\n",
    "eval_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_train.pt'), map_location=\"cpu\")[-1000:]\n",
    "#test_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_test.pt'), map_location=\"cpu\")\n",
    "\n",
    "print(train_data.shape)\n",
    "train_data = train_data.permute(0, 1, 4, 2, 3)\n",
    "eval_data = eval_data.permute(0, 1, 4, 2, 3)\n",
    "#test_data = test_data.permute(0, 1, 4, 2, 3)\n",
    "print(train_data.shape)\n",
    "\n",
    "train_seq_mask = torch.ones(train_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "eval_seq_mask = torch.ones(eval_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "#test_seq_mask = torch.ones(test_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "train_pix_mask = torch.ones_like(train_data, requires_grad=False).type(torch.bool)\n",
    "eval_pix_mask = torch.ones_like(eval_data, requires_grad=False).type(torch.bool)\n",
    "#test_pix_mask = torch.ones_like(test_data, requires_grad=False).type(torch.bool)\n",
    "\n",
    "train_dataset = My_MaskedDataset(train_data, train_seq_mask, train_pix_mask)\n",
    "eval_dataset = My_MaskedDataset(eval_data, eval_seq_mask, eval_pix_mask)\n",
    "#test_dataset = My_MaskedDataset(test_data, test_seq_mask, test_pix_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (decoder): Decoder_Chairs(\n",
       "    (fc): Linear(in_features=192, out_features=2048, bias=True)\n",
       "    (layers): Sequential(\n",
       "      (0): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (8): LeakyReLU(negative_slope=0.01)\n",
       "      (9): ConvTranspose2d(64, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (10): LeakyReLU(negative_slope=0.01)\n",
       "      (11): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder_Chairs(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (6): LeakyReLU(negative_slope=0.01)\n",
       "      (7): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (11): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (12): ResBlock(\n",
       "        (conv_block): Sequential(\n",
       "          (0): ReLU()\n",
       "          (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (embedding): Linear(in_features=2048, out_features=192, bias=True)\n",
       "    (log_var): Linear(in_features=2048, out_features=192, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder = Encoder_Chairs(config)\n",
    "# decoder = Decoder_Chairs(config)\n",
    "# vae = LVAE_IAF(config, encoder, decoder)\n",
    "\n",
    "device = 'cuda'\n",
    "vae = VAE.load_from_folder('pre-trained_vae/VAE_training_2024-05-22_14-43-10/final_model').to(device)\n",
    "vae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_p_j_hat(j, z, zT_samples, alpha_bars, tau = 125):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    alpha_bar_j = alpha_bars[j*tau]\n",
    "\n",
    "    log_density = torch.sum( (z - zT_samples)**2 / (2 * (1 - alpha_bar_j)**2), dim=-1)\n",
    "\n",
    "    return log_density.mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(374.5630)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zT_samples = torch.load('zT_samples.pt').to('cpu')\n",
    "z = torch.randn(1, 192)\n",
    "alpha_bars = 0.5*torch.ones(1000)\n",
    "log_p_j_hat(1, z, zT_samples, alpha_bars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "#ergonomiser le Riemaniann sampling\n",
    "- inspecter LVIAF\n",
    "- implém sequential forward diff\n",
    "- train !\n",
    "\n",
    "Possible pain points : le diffusion backward très bof (at least dans les intemédiaires). Après c'est du sampling... wait and see\n",
    "\n",
    "\n",
    "Voir s'il faut baisser la dim latent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

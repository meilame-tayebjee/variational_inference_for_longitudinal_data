{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'variational_inference_for_longitudinal_data/'\n",
      "/users/eleves-a/2020/meilame.tayebjee/variational_inference_for_longitudinal_data\n",
      "Fri May 31 14:39:42 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A2000 12GB          Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   35C    P8             11W /   70W |    1142MiB /  12282MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    312721      C   ...lame.tayebjee/miniconda3/bin/python        368MiB |\n",
      "|    0   N/A  N/A    628998      G   /usr/libexec/Xorg                              94MiB |\n",
      "|    0   N/A  N/A    629024      G   /usr/bin/gnome-shell                           18MiB |\n",
      "|    0   N/A  N/A    664318      C   ...lame.tayebjee/miniconda3/bin/python        216MiB |\n",
      "|    0   N/A  N/A    664320      C   ...lame.tayebjee/miniconda3/bin/python        170MiB |\n",
      "|    0   N/A  N/A    664376      C   ...lame.tayebjee/miniconda3/bin/python        258MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "%cd variational_inference_for_longitudinal_data/\n",
    "sys.path.append('diffusion/stable_diffusion/')\n",
    "sys.path.append('diffusion/stable_diffusion/model/')\n",
    "sys.path.append('lib/src/')\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from diffusion.stable_diffusion.latent_diffusion import MyLatentDiffusion, LitLDM\n",
    "from diffusion.stable_diffusion.model.autoencoder import Autoencoder\n",
    "from diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder\n",
    "from diffusion.stable_diffusion.model.unet import UNetModel, _test_time_embeddings\n",
    "\n",
    "from lib.src.pythae.models import VAE\n",
    "from lib.src.pythae.models import LVAE_IAF, LVAE_IAF_Config, AutoModel\n",
    "from lib.src.pythae.models.nn.base_architectures import BaseDecoder\n",
    "from lib.scripts.utils import Encoder_Chairs,Decoder_Chairs, My_Dataset, My_MaskedDataset, make_batched_masks\n",
    "\n",
    "\n",
    "\n",
    "from geometric_perspective_on_vaes.sampling import build_metrics, log_pi\n",
    "\n",
    "def load_config_unet(config):\n",
    "    return UNetModel(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        channels=config['channels'],\n",
    "        n_res_blocks=config['n_res_blocks'],\n",
    "        attention_levels=config['attention_levels'],\n",
    "        channel_multipliers=config['channel_multipliers'],\n",
    "        n_heads=config['n_heads'],\n",
    "    )\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 8, 64, 64, 3])\n",
      "torch.Size([8000, 8, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA = 'lib/my_data/sprites/Sprites_train.pt'\n",
    "\n",
    "\n",
    "train_data = torch.load(os.path.join(PATH_DATA))[:-1000]\n",
    "eval_data = torch.load(os.path.join(PATH_DATA), map_location=\"cpu\")[-1000:]\n",
    "#test_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_test.pt'), map_location=\"cpu\")\n",
    "\n",
    "print(train_data.shape)\n",
    "train_data = train_data.permute(0, 1, 4, 2, 3)\n",
    "eval_data = eval_data.permute(0, 1, 4, 2, 3)\n",
    "#test_data = test_data.permute(0, 1, 4, 2, 3)\n",
    "print(train_data.shape)\n",
    "\n",
    "train_seq_mask = torch.ones(train_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "eval_seq_mask = torch.ones(eval_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "#test_seq_mask = torch.ones(test_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "train_pix_mask = torch.ones_like(train_data, requires_grad=False).type(torch.bool)\n",
    "eval_pix_mask = torch.ones_like(eval_data, requires_grad=False).type(torch.bool)\n",
    "#test_pix_mask = torch.ones_like(test_data, requires_grad=False).type(torch.bool)\n",
    "\n",
    "train_dataset = My_MaskedDataset(train_data, train_seq_mask, train_pix_mask)\n",
    "eval_dataset = My_MaskedDataset(eval_data, eval_seq_mask, eval_pix_mask)\n",
    "#test_dataset = My_MaskedDataset(test_data, test_seq_mask, test_pix_mask)\n",
    "\n",
    "\n",
    "NUM_WORKERS = 12\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=200, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(eval_data, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dim: 12\n",
      "Number of parameters in the diffusion model:  2223043\n"
     ]
    }
   ],
   "source": [
    "# encoder = Encoder_Chairs(config)\n",
    "# decoder = Decoder_Chairs(config)\n",
    "# vae = LVAE_IAF(config, encoder, decoder)\n",
    "PATH_VAE_FOLDER = 'pre-trained_vae/VAE_training_2024-05-30_17-33-43-latdim12/final_model'\n",
    "PATH_DIFFUSION_CKPT = 'ldm/lightning_logs/version_30/checkpoints/epoch=44-step=900.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "vae = VAE.load_from_folder(PATH_VAE_FOLDER).to(device)\n",
    "vae.eval()\n",
    "\n",
    "# in_channels = 3\n",
    "# out_channels = 3\n",
    "# channels = 32\n",
    "# n_res_blocks = 2\n",
    "# attention_levels = [2]\n",
    "# channel_multipliers = (1, 2, 4)\n",
    "# n_heads = 16\n",
    "\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "channels = 64\n",
    "n_res_blocks = 4\n",
    "attention_levels = [0]\n",
    "channel_multipliers = [1]\n",
    "n_heads = 4\n",
    "\n",
    "unet_config = {\n",
    "    'in_channels': in_channels,\n",
    "    'out_channels': out_channels,\n",
    "    'channels': channels,\n",
    "    'n_res_blocks': n_res_blocks,\n",
    "    'attention_levels': attention_levels,\n",
    "    'channel_multipliers': channel_multipliers,\n",
    "    'n_heads': n_heads,\n",
    "}\n",
    "\n",
    "unet = load_config_unet(unet_config)\n",
    "\n",
    "latent_scaling_factor = 1\n",
    "n_steps = 1000\n",
    "linear_start =  0.00085\n",
    "linear_end = 0.012\n",
    "\n",
    "input_dim = (3, 64, 64)\n",
    "f = 32 #subsampling factor\n",
    "latent_dim = 3* (64 // f) * (64 // f)\n",
    "print('Latent dim:', latent_dim)\n",
    "\n",
    "\n",
    "latent_diffusion = MyLatentDiffusion(unet, None, latent_scaling_factor, n_steps, linear_start, linear_end)\n",
    "print('Number of parameters in the diffusion model: ', sum(p.numel() for p in latent_diffusion.parameters() if p.requires_grad))\n",
    "\n",
    "model = LitLDM.load_from_checkpoint(PATH_DIFFUSION_CKPT, ldm = latent_diffusion, vae = vae, latent_dim = latent_dim, lr = 6e-4).to('cuda')\n",
    "diffusion = model.ldm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveG(vae, train_data, verbose = False, device = 'cpu'):\n",
    "    last_obs_train = train_data[:, -1, :, :, :].to(device)\n",
    "    loader = torch.utils.data.DataLoader(last_obs_train, batch_size=200, shuffle=False)\n",
    "    mu = []\n",
    "    log_var = []\n",
    "    vae = vae.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ , x in enumerate(loader):\n",
    "\n",
    "            data = x.data\n",
    "\n",
    "            out = vae.encoder(data.to(device))\n",
    "\n",
    "            mu_data, log_var_data = out.embedding, out.log_covariance\n",
    "\n",
    "            mu.append(mu_data)\n",
    "            log_var.append(log_var_data)\n",
    "\n",
    "    mu = torch.cat(mu)\n",
    "    log_var = torch.cat(log_var)\n",
    "\n",
    "    if verbose:\n",
    "        print('Running Kmedoids')\n",
    "        print(mu.shape)\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=100).fit(mu.detach().cpu().numpy())\n",
    "    medoids = torch.tensor(kmedoids.cluster_centers_).to(device)\n",
    "    centroids_idx = kmedoids.medoid_indices_ #\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finding temperature\")\n",
    "        \n",
    "    eps_lf = 0.01\n",
    "    lbd = 0.01\n",
    "    T = 0\n",
    "    T_is = []\n",
    "    for i in range(len(medoids)-1):\n",
    "        mask = torch.tensor([k for k in range(len(medoids)) if k != i])\n",
    "        dist = torch.norm(medoids[i].unsqueeze(0) - medoids[mask], dim=-1)\n",
    "        T_i =torch.min(dist, dim=0)[0]\n",
    "        T_is.append(T_i.item())\n",
    "\n",
    "    T = np.max(T_is)\n",
    "\n",
    "    if verbose: \n",
    "        print('Best temperature found: ', T)\n",
    "        print('Building metric')\n",
    "\n",
    "    vae = build_metrics(vae, mu, log_var, centroids_idx, T=T, lbd=lbd)\n",
    "\n",
    "    return vae.G_sampl, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_p_j_hat(j, z, zT_samples, alpha_bars, tau = 125):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    alpha_bar_j = alpha_bars[j*tau]\n",
    "    log_density = torch.sum( (z - zT_samples)**2 / (2 * (1 - alpha_bar_j)**2), dim=-1)\n",
    "\n",
    "    return log_density.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40.8740)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zT_samples = torch.load('zT_samples.pt').to('cpu')\n",
    "z = torch.randn(1, latent_dim)\n",
    "alpha_bars = 0.5*torch.ones(1000)\n",
    "log_p_j_hat(1, z, zT_samples, alpha_bars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Kmedoids\n",
      "torch.Size([8000, 12])\n",
      "Finding temperature\n",
      "Best temperature found:  1.8156334161758423\n",
      "Building metric\n"
     ]
    }
   ],
   "source": [
    "G, log_var= retrieveG(vae, train_data, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4570)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_var > 3*1e-2).sum()/ 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-27.6306], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = vae.encoder(train_data[0, -1, :, :, :].unsqueeze(0).to('cpu')).embedding\n",
    "log_pi(vae, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "#ergonomiser le Riemaniann sampling\n",
    "- inspecter LVIAF\n",
    "- train !\n",
    "\n",
    "Possible pain points : le diffusion backward très bof (at least dans les intemédiaires). Après c'est du sampling... wait and see\n",
    "\n",
    "30/05\n",
    "Le problème du vanishing G a été résolu en passant à une dim latente de 12.\n",
    "Il faut améliorer le diffusion model !\n",
    "Eventuellemnt passer à du 16, puis 1, 4, 4 pour avoir des étages ce qui  n'est pas le cas now. Checker si on a pas de vansihing G."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

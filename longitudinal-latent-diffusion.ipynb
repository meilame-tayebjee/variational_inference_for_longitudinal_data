{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('diffusion/stable_diffusion/')\n",
    "sys.path.append('diffusion/stable_diffusion/model/')\n",
    "sys.path.append('lib/src/')\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from diffusion.stable_diffusion.latent_diffusion import LatentDiffusion\n",
    "from diffusion.stable_diffusion.model.autoencoder import Autoencoder\n",
    "from diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder\n",
    "from diffusion.stable_diffusion.model.unet import UNetModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from lib.src.pythae.models import LVAE_IAF, LVAE_IAF_Config, AutoModel\n",
    "from lib.src.pythae.models.nn.base_architectures import BaseDecoder\n",
    "from lib.scripts.utils import Encoder_Chairs,Decoder_Chairs, My_Dataset, My_MaskedDataset, make_batched_masks\n",
    "\n",
    "def load_config_unet(config):\n",
    "    return UNetModel(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        channels=config['channels'],\n",
    "        n_res_blocks=config['n_res_blocks'],\n",
    "        attention_levels=config['attention_levels'],\n",
    "        channel_multipliers=config['channel_multipliers'],\n",
    "        n_heads=config['n_heads'],\n",
    "    )\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_train.pt'))[:-1000]\n",
    "eval_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_train.pt'), map_location=\"cpu\")[-1000:]\n",
    "test_data = torch.load(os.path.join('lib/my_data/sprites/Sprites_test.pt'), map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = (3, 64, 64)\n",
    "latent_dim = 16\n",
    "beta = 1\n",
    "n_hidden_in_made = 3\n",
    "n_made_blocks = 2\n",
    "warmup = 10\n",
    "context_dim = None\n",
    "prior = 'vamp'\n",
    "posterior = 'iaf'\n",
    "vamp_number_components= 500\n",
    "linear_scheduling_steps = 10\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "learning_rate=  1e-3 \n",
    "steps_saving = None\n",
    "steps_predict = 100\n",
    "shuffle_data = True\n",
    "\n",
    "config = LVAE_IAF_Config(\n",
    "    input_dim=input_dim,\n",
    "    n_obs_per_ind=8,\n",
    "    latent_dim=latent_dim,\n",
    "    beta=beta,\n",
    "    n_hidden_in_made=n_hidden_in_made,\n",
    "    n_made_blocks=n_made_blocks,\n",
    "    warmup=warmup,\n",
    "    context_dim=context_dim,\n",
    "    prior=prior,\n",
    "    posterior=posterior,\n",
    "    vamp_number_components=vamp_number_components,\n",
    "    linear_scheduling_steps=linear_scheduling_steps\n",
    ")\n",
    "\n",
    "\n",
    "encoder = Encoder_Chairs(config)\n",
    "decoder = Decoder_Chairs(config)\n",
    "vae = LVAE_IAF(config, encoder, decoder)\n",
    "\n",
    "vae.load_state_dict(torch.load('lib/dummy_output_dir/LVAE_IAF_training_2024-05-01_17-22-06/final_model/model.pt')['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 256, 256])\n",
      "4864\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(1, 3, 256, 256)\n",
    "conv = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, padding=2)\n",
    "\n",
    "print(conv(z).shape)\n",
    "print(sum(p.numel() for p in conv.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_time_emb 256\n",
      "Channels list [64, 128]\n",
      "0\n",
      "1\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "Entering middle block  torch.Size([1, 128, 128, 128])\n",
      "Shap entering the trans:  torch.Size([1, 128, 128, 128])\n",
      "Middle block done  torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "in_channels = 3\n",
    "out_channels = 1\n",
    "channels = 64\n",
    "n_res_blocks = 2\n",
    "attention_levels = [6]\n",
    "channel_multipliers = (1, 2)\n",
    "n_heads = 2\n",
    "\n",
    "unet_config = {\n",
    "    'in_channels': in_channels,\n",
    "    'out_channels': out_channels,\n",
    "    'channels': channels,\n",
    "    'n_res_blocks': n_res_blocks,\n",
    "    'attention_levels': attention_levels,\n",
    "    'channel_multipliers': channel_multipliers,\n",
    "    'n_heads': n_heads,\n",
    "}\n",
    "\n",
    "unet = load_config_unet(unet_config)\n",
    "z = torch.randn(1, 3, 256, 256)\n",
    "empty_prompt_embed = None\n",
    "unet(z, torch.tensor([10]), empty_prompt_embed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "from diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder\n",
    "\n",
    "clip = CLIPTextEmbedder(device = 'cuda')\n",
    "prompt = ''\n",
    "empty_prompt_embed = clip(prompt)\n",
    "empty_prompt_embed = empty_prompt_embed.to('cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
